{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd gdrive/MyDrive/repos/knowledge-graph-completion/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install colab-xterm\n",
    "!pip install transformers==4.28.0 accelerate\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS ###\n",
    "\n",
    "DATASETS = {\n",
    "    \"FB15k-237-DECODE-ONLY-LABEL\": \"data/data_processed/FB15k-237/decode_only_label/\",\n",
    "}\n",
    "MODELS = {\n",
    "    \"bart-small\": \"lucadiliello/bart-small\",\n",
    "    \"bart-base\": \"facebook/bart-base\",\n",
    "    \"bart-large\": \"facebook/bart-large\",\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "DATASET = \"FB15k-237-DECODE-ONLY-LABEL\"\n",
    "MODEL = \"bart-small\"\n",
    "MODEL_NAME = MODEL + \"_\" + DATASET\n",
    "MODEL_PATH = f\"models/{MODEL_NAME}\"\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "# Training\n",
    "params = {\n",
    "    # Dir\n",
    "    \"output_dir\": f\"models/{MODEL_NAME}/\",\n",
    "    # Batch\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    # Learning rate\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"seed\": 42,\n",
    "    # Epochs\n",
    "    \"num_train_epochs\": 50,\n",
    "    # Logging\n",
    "    \"logging_dir\": f\"models/{MODEL_NAME}/logs\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 10,\n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"eval_steps\": 1,\n",
    "    # Checkpoint\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 2,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"ddp_find_unused_parameters\": False,\n",
    "    \"warmup_steps\": 2,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = (\n",
    "    BartForConditionalGeneration.from_pretrained(MODELS[MODEL], use_cache=False)\n",
    "    .cuda()\n",
    "    .float()\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(MODELS[MODEL])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasetkgc import DatasetKGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds_DEV.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds_DEV.pth\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "epochs = params[\"num_train_epochs\"]\n",
    "epoch_accuracy_frequency = [\"eval_steps\"]\n",
    "lr = params[\"learning_rate\"]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds, valid_ds = (\n",
    "    DataLoader(\n",
    "        train_ds, batch_size=params[\"per_device_train_batch_size\"], shuffle=False\n",
    "    ),\n",
    "    DataLoader(\n",
    "        valid_ds, batch_size=params[\"per_device_eval_batch_size\"], shuffle=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if checkpoint exists\n",
    "\n",
    "if os.path.exists(f\"{params['output_dir']}loop_trainer/checkpoint/\"):\n",
    "    checkpoint_path = (\n",
    "        f\"{params['output_dir']}loop_trainer/checkpoint/\"\n",
    "        + sorted(os.listdir(f\"{params['output_dir']}loop_trainer/checkpoint/\"))[-1]\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    train_epoch_loss = checkpoint[\"loss\"]\n",
    "    train_losses = checkpoint[\"train_losses\"]\n",
    "    valid_losses = checkpoint[\"valid_losses\"]\n",
    "\n",
    "else:\n",
    "    os.makedirs(f\"{params['output_dir']}/loop_trainer/checkpoint/\", exist_ok=True)\n",
    "    start_epoch = 0\n",
    "    train_epoch_loss = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"num_train_epochs\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pbar = tqdm(range(start_epoch + 1, params[\"num_train_epochs\"] + 1), desc=\"Epochs\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    pbar.set_description(\"Epoch %s\" % epoch)\n",
    "    pbar.refresh()\n",
    "\n",
    "    # Checkpoint\n",
    "    if epoch % params[\"save_steps\"] == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": train_epoch_loss,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"valid_losses\": valid_losses,\n",
    "            },\n",
    "            f\"{params['output_dir']}/loop_trainer/checkpoint/epoch_{epoch}.pth\",\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_epoch_loss = 0\n",
    "\n",
    "    for batch in train_ds:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids, attention_mask, labels = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"labels\"],\n",
    "        )\n",
    "        label = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids.to(device),\n",
    "            labels=label.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "\n",
    "    # Flag to avoid multiple trains\n",
    "    start_epoch += 1\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if epoch % params[\"eval_steps\"] == 0:\n",
    "        valid_loss = 0\n",
    "\n",
    "        for batch in valid_ds:\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"labels\"],\n",
    "            )\n",
    "            label = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids.to(device),\n",
    "                labels=label.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    else:\n",
    "        valid_losses.append(0)\n",
    "\n",
    "    pbar.set_postfix(loss=train_epoch_loss)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{params['output_dir']}loop_trainer/trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds_DEV.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds_DEV.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    global loss_values\n",
    "    loss = float(eval_pred[\"loss\"])\n",
    "    loss_values.append(eval_pred)\n",
    "\n",
    "    return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "os.makedirs(params[\"output_dir\"] + \"hf_trainer/\", exist_ok=True)\n",
    "os.makedirs(params[\"output_dir\"] + \"hf_trainer/logs/\", exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Dir\n",
    "    output_dir=params[\"output_dir\"] + \"hf_trainer/checkpoint/\",\n",
    "    # Batch\n",
    "    per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "    # Learning Rate\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    seed=params[\"seed\"],\n",
    "    # Epoch\n",
    "    num_train_epochs=params[\"num_train_epochs\"],\n",
    "    # logging\n",
    "    logging_dir=params[\"output_dir\"] + \"hf_trainer/logs\",\n",
    "    logging_strategy=params[\"logging_strategy\"],\n",
    "    logging_steps=params[\"logging_steps\"],\n",
    "    # Evaluation\n",
    "    # evaluation_strategy=params[\"evaluation_strategy\"],\n",
    "    # eval_steps=params[\"eval_steps\"],\n",
    "    # Checkpoint\n",
    "    save_strategy=params[\"save_strategy\"],\n",
    "    save_steps=params[\"save_steps\"],\n",
    "    save_total_limit=params[\"save_total_limit\"],\n",
    "    # pretraining\n",
    "    ddp_find_unused_parameters=params[\"ddp_find_unused_parameters\"],\n",
    "    warmup_steps=params[\"warmup_steps\"],\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    # test\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfiy if checkpoint exists\n",
    "if os.path.exists(f\"{params['output_dir']}hf_trainer/checkpoint/\"):\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{params['output_dir']}hf_trainer/trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(f\"{f'{MODEL} - {DATASET}'} train\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss - Cross Entropy\")\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "\n",
    "plt.text(\n",
    "    max(xmin, xmax) * 0.9,\n",
    "    max(ymin, ymax) * 0.9,\n",
    "    f'epochs = {params[\"num_train_epochs\"]}\\nlr={params[\"learning_rate\"]}',\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i[\"epoch\"] for i in trainer.state.log_history[:-1]]\n",
    "y = [i[\"loss\"] for i in trainer.state.log_history[:-1]]\n",
    "plt.title(f\"{f'{MODEL} - {DATASET}'} Loss\")\n",
    "plt.text(\n",
    "    max(x) * 0.99,\n",
    "    max(y) * 0.99,\n",
    "    f'epochs = {params[\"num_train_epochs\"]}\\nlr={params[\"learning_rate\"]}',\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    ")\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss - Cross Entropy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
