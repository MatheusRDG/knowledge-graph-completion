{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS ###\n",
    "\n",
    "DATASETS = {\n",
    "    \"FB15k-237-DECODE-ONLY-LABEL\": \"data/data_processed/FB15k-237/decode_only_label/\",\n",
    "}\n",
    "MODELS = {\n",
    "    \"bart-small\": \"lucadiliello/bart-small\",\n",
    "    \"bart-base\": \"facebook/bart-base\",\n",
    "    \"bart-large\": \"facebook/bart-large\",\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "DATASET = \"FB15k-237-DECODE-ONLY-LABEL\"\n",
    "MODEL = \"bart-small\"\n",
    "MODEL_NAME = MODEL + \"_\" + DATASET\n",
    "MODEL_PATH = f\"models/{MODEL_NAME}\"\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "# Training\n",
    "params = {\n",
    "    # Dir\n",
    "    \"output_dir\": f\"models/{MODEL_NAME}/\",\n",
    "    # Batch\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    # Learning rate\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"seed\": 42,\n",
    "    # Epochs\n",
    "    \"num_train_epochs\": 50,\n",
    "    # Logging\n",
    "    \"logging_dir\": \"model/logs\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 10,\n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"eval_steps\": 1,\n",
    "    # Checkpoint\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 2,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"ddp_find_unused_parameters\": False,\n",
    "    \"warmup_steps\": 2,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = (\n",
    "    BartForConditionalGeneration.from_pretrained(MODEL_PATH).cuda().float().to(device)\n",
    ")\n",
    "tokenizer = BartTokenizer.from_pretrained(MODELS[MODEL])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasetkgc import DatasetKGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds, valid_ds = (\n",
    "    DataLoader(\n",
    "        train_ds, batch_size=params[\"per_device_train_batch_size\"], shuffle=False\n",
    "    ),\n",
    "    DataLoader(\n",
    "        valid_ds, batch_size=params[\"per_device_eval_batch_size\"], shuffle=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Iterative Playground\n",
    "\n",
    "define idx variable and can run standlone predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "idx = 0\n",
    "_valid_ds = next(iter(valid_ds))\n",
    "input_ids = _valid_ds[\"input_ids\"][idx].to(device)\n",
    "attention_mask = _valid_ds[\"attention_mask\"][idx].to(device)\n",
    "labels = _valid_ds[\"labels\"][idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Sample input: \")\n",
    "    print(tokenizer.decode(input_ids).replace(\"<pad>\", \"\"), \"\\n\")\n",
    "\n",
    "    print(\"Expected output: \")\n",
    "    print(tokenizer.decode(labels, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "    print(\"Model Output: \")\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(input_ids.reshape(1, -1), max_length=MAX_LENGTH)[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hits\n",
    "\n",
    "This metric measure the distance between the representation of label and output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cosine_similarity(embd_i, embd_j):\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    return cos(embd_i, embd_j)\n",
    "\n",
    "\n",
    "def hits_sim(model, tokenizer, valid_ds: dict):\n",
    "    results = []\n",
    "\n",
    "    data_size = valid_ds[\"input_ids\"].shape[0]\n",
    "\n",
    "    for i in range(data_size):\n",
    "        text_label = tokenizer.decode(valid_ds[\"labels\"][i], skip_special_tokens=True)\n",
    "        embedding_label = model(\n",
    "            tokenizer.encode(\n",
    "                text_label, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
    "            )\n",
    "            .reshape(1, -1)\n",
    "            .to(device)\n",
    "        ).encoder_last_hidden_state\n",
    "        embedding_label = torch.mean(embedding_label, dim=1)\n",
    "\n",
    "        token_ids_output = model.generate(\n",
    "            valid_ds[\"input_ids\"][i].to(device).reshape(1, -1), max_length=MAX_LENGTH\n",
    "        )[0]\n",
    "        text_output = tokenizer.decode(token_ids_output, skip_special_tokens=True)\n",
    "\n",
    "        embeddings_output = model(\n",
    "            tokenizer.encode(\n",
    "                text_output, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
    "            )\n",
    "            .to(device)\n",
    "            .reshape(1, -1)\n",
    "        ).encoder_last_hidden_state\n",
    "        embeddings_output = torch.mean(embeddings_output, dim=1)\n",
    "\n",
    "        similarity = cosine_similarity(embedding_label, embeddings_output)[0].item()\n",
    "\n",
    "        # print(\"Label: \", text_label)\n",
    "        # print(\"Output: \", text_output)\n",
    "        # print(\"Similarity: \", similarity)\n",
    "        # print(\"---------------------------------------------------------\")\n",
    "\n",
    "        results.append((text_label, text_output, similarity))\n",
    "\n",
    "    results = pd.DataFrame(results, columns=[\"label\", \"output\", \"similarity\"])\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of cosine similarity of label and output encoded by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "idx = 0\n",
    "_valid_ds = next(iter(valid_ds))\n",
    "input_ids = _valid_ds[\"input_ids\"][idx].to(device)\n",
    "attention_mask = _valid_ds[\"attention_mask\"][idx].to(device)\n",
    "labels = _valid_ds[\"labels\"][idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_label = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(text_label)\n",
    "\n",
    "embedding_label = model(\n",
    "    tokenizer.encode(\n",
    "        text_label, padding=\"max_length\", max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
    "    )\n",
    "    .reshape(1, -1)\n",
    "    .to(device)\n",
    ").encoder_last_hidden_state\n",
    "embedding_label = torch.mean(embedding_label, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids_output = model.generate(input_ids.reshape(1, -1), max_length=MAX_LENGTH)[0]\n",
    "text_output = tokenizer.decode(token_ids_output, skip_special_tokens=True)\n",
    "print(text_output)\n",
    "\n",
    "embeddings_output = model(\n",
    "    tokenizer.encode(\n",
    "        text_output, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "    .to(device)\n",
    "    .reshape(1, -1)\n",
    ").encoder_last_hidden_state\n",
    "embeddings_output = torch.mean(embeddings_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embedding_label, embeddings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = 50\n",
    "results = hits(model, tokenizer, valid_ds[:validation_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = tokenizer.decode(valid_ds[0][\"input_ids\"], skip_special_tokens=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Hits Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_data_to_text(input_ids, tokenizer=tokenizer):\n",
    "    text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "    # 3 -> len(<s>), 7 -> len(<mask>.)\n",
    "\n",
    "    return text[3 : text.find(tokenizer.mask_token) + 7]\n",
    "\n",
    "\n",
    "def generate_beam_search(model, tokenizer, text, beam_size=5, max_length=128):\n",
    "    # Text Example: My name is <mask>.\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    beam_outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=beam_size,\n",
    "        num_return_sequences=beam_size,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        tokenizer.decode(beam_output, skip_special_tokens=True)\n",
    "        for beam_output in beam_outputs\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_hits(model, tokenizer, valid_ds, beam_size=5, max_length=128, debug=False):\n",
    "    results = []\n",
    "    hits = 0\n",
    "    data_size = valid_ds[\"input_ids\"].shape[0]\n",
    "\n",
    "    for i in range(data_size):\n",
    "        text = training_data_to_text(valid_ds[\"input_ids\"][i], tokenizer=tokenizer)\n",
    "        label = tokenizer.decode(valid_ds[\"labels\"][i], skip_special_tokens=True)\n",
    "        output_list = generate_beam_search(\n",
    "            text=text,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            beam_size=beam_size,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        if label.lower() in list(map(lambda x: x.lower(), output_list)):\n",
    "            hits += 1\n",
    "\n",
    "        results.append((text, label, output_list))\n",
    "\n",
    "        if debug:\n",
    "            print(\"Text: \", text)\n",
    "            print(\"Label: \", label)\n",
    "            print(\"Output: \", output_list)\n",
    "            print()\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"text\", \"label\", \"output_list\"]), hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results beam size 1\n",
    "batch = 1000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=1, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results beam size 3\n",
    "batch = 1000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=3, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results beam size 10\n",
    "batch = 1000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=10, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "TXT = \"Brazil capital is São <mask>.\"\n",
    "\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(input_ids.to(device)).logits\n",
    "\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(10)\n",
    "\n",
    "tokenizer.decode(predictions).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "TXT = \"Piano is a type of <mask>.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(input_ids.to(device)).logits\n",
    "\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "\n",
    "predicted_tokens = tokenizer.batch_decode(predictions)\n",
    "\n",
    "suggested_words = tokenizer.decode(predictions).split()\n",
    "\n",
    "# Gerar 5 preenchimentos de 2 tokens cada\n",
    "combinations = [\n",
    "    (suggested_words[i], suggested_words[j]) for i in range(5) for j in range(i + 1, 5)\n",
    "]\n",
    "\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT = \"Piano is a type of <mask>.\"\n",
    "\n",
    "# Definir hiperparâmetros da busca beam search\n",
    "num_beams = 5\n",
    "num_tokens = 10\n",
    "\n",
    "# Tokenizar a entrada\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Gerar preenchimentos usando busca beam search\n",
    "output = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=input_ids.shape[-1] + num_tokens,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=num_beams,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# Decodificar as sequências geradas\n",
    "decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "# Gerar combinações de 2 tokens para as sequências decodificadas\n",
    "print(decoded_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
