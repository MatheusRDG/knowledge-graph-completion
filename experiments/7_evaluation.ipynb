{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS ###\n",
    "\n",
    "DATASETS = {\n",
    "    \"FB15k-237-DECODE-ONLY-LABEL\": \"data/data_processed/FB15k-237/decode_only_label/\",\n",
    "    \"ALL-DATA-DECODE-ONLY-LABEL\": \"data/data_processed/FB15k_FB15k237_WN18_WN18RR/\",\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    \"bart-small\": \"lucadiliello/bart-small\",\n",
    "    \"bart-base\": \"facebook/bart-base\",\n",
    "    \"bart-large\": \"facebook/bart-large\",\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "DATASET = \"ALL-DATA-DECODE-ONLY-LABEL\"\n",
    "MODEL = \"bart-base\"\n",
    "MODEL_NAME = MODEL + \"_\" + DATASET\n",
    "MODEL_PATH = f\"models/{MODEL_NAME}/hf_trainer/trained_model/\"\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "# Training\n",
    "params = {\n",
    "    # Dir\n",
    "    \"output_dir\": f\"models/{MODEL_NAME}/\",\n",
    "    # Batch\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    # Learning rate\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"seed\": 42,\n",
    "    # Epochs\n",
    "    \"num_train_epochs\": 50,\n",
    "    # Logging\n",
    "    \"logging_dir\": \"model/logs\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 10,\n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"eval_steps\": 1,\n",
    "    # Checkpoint\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 2,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"ddp_find_unused_parameters\": False,\n",
    "    \"warmup_steps\": 2,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = (\n",
    "    BartForConditionalGeneration.from_pretrained(MODEL_PATH).cuda().float().to(device)\n",
    ")\n",
    "tokenizer = BartTokenizer.from_pretrained(MODELS[MODEL])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasetkgc import DatasetKGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds, valid_ds = (\n",
    "    DataLoader(\n",
    "        train_ds, batch_size=params[\"per_device_train_batch_size\"], shuffle=False\n",
    "    ),\n",
    "    DataLoader(\n",
    "        valid_ds, batch_size=params[\"per_device_eval_batch_size\"], shuffle=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - Iterative Playground\n",
    "\n",
    "define idx variable and can run standlone predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "idx = 1\n",
    "_valid_ds = next(iter(valid_ds))\n",
    "input_ids = _valid_ds[\"input_ids\"][idx].to(device)\n",
    "attention_mask = _valid_ds[\"attention_mask\"][idx].to(device)\n",
    "labels = _valid_ds[\"labels\"][idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: \n",
      "<s>musical film has films in this genre of Beauty and the Beast. war film has films in this genre of The Living Daylights. satire has films in this genre of<mask>.</s> \n",
      "\n",
      "Expected output: \n",
      "The Simpsons Movie \n",
      "\n",
      "Model Output: \n",
      "</s><s>The Manchurian Candidate</s>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Sample input: \")\n",
    "    print(tokenizer.decode(input_ids).replace(\"<pad>\", \"\"), \"\\n\")\n",
    "\n",
    "    print(\"Expected output: \")\n",
    "    print(tokenizer.decode(labels, skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "    print(\"Model Output: \")\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(input_ids.reshape(1, -1), max_length=MAX_LENGTH)[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hits\n",
    "\n",
    "This metric measure the distance between the representation of label and output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cosine_similarity(embd_i, embd_j):\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    return cos(embd_i, embd_j)\n",
    "\n",
    "\n",
    "def hits_sim(model, tokenizer, valid_ds: dict):\n",
    "    results = []\n",
    "\n",
    "    data_size = valid_ds[\"input_ids\"].shape[0]\n",
    "\n",
    "    for i in range(data_size):\n",
    "        text_label = tokenizer.decode(valid_ds[\"labels\"][i], skip_special_tokens=True)\n",
    "        embedding_label = model(\n",
    "            tokenizer.encode(\n",
    "                text_label, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
    "            )\n",
    "            .reshape(1, -1)\n",
    "            .to(device)\n",
    "        ).encoder_last_hidden_state\n",
    "        embedding_label = torch.mean(embedding_label, dim=1)\n",
    "\n",
    "        token_ids_output = model.generate(\n",
    "            valid_ds[\"input_ids\"][i].to(device).reshape(1, -1), max_length=MAX_LENGTH\n",
    "        )[0]\n",
    "        text_output = tokenizer.decode(token_ids_output, skip_special_tokens=True)\n",
    "\n",
    "        embeddings_output = model(\n",
    "            tokenizer.encode(\n",
    "                text_output, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
    "            )\n",
    "            .to(device)\n",
    "            .reshape(1, -1)\n",
    "        ).encoder_last_hidden_state\n",
    "        embeddings_output = torch.mean(embeddings_output, dim=1)\n",
    "\n",
    "        similarity = cosine_similarity(embedding_label, embeddings_output)[0].item()\n",
    "\n",
    "        # print(\"Label: \", text_label)\n",
    "        # print(\"Output: \", text_output)\n",
    "        # print(\"Similarity: \", similarity)\n",
    "        # print(\"---------------------------------------------------------\")\n",
    "\n",
    "        results.append((text_label, text_output, similarity))\n",
    "\n",
    "    results = pd.DataFrame(results, columns=[\"label\", \"output\", \"similarity\"])\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of cosine similarity of label and output encoded by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "idx = 0\n",
    "_valid_ds = next(iter(valid_ds))\n",
    "input_ids = _valid_ds[\"input_ids\"][idx].to(device)\n",
    "attention_mask = _valid_ds[\"attention_mask\"][idx].to(device)\n",
    "labels = _valid_ds[\"labels\"][idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_label = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "print(text_label)\n",
    "\n",
    "embedding_label = model(\n",
    "    tokenizer.encode(\n",
    "        text_label, padding=\"max_length\", max_length=MAX_LENGTH, return_tensors=\"pt\"\n",
    "    )\n",
    "    .reshape(1, -1)\n",
    "    .to(device)\n",
    ").encoder_last_hidden_state\n",
    "embedding_label = torch.mean(embedding_label, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids_output = model.generate(input_ids.reshape(1, -1), max_length=MAX_LENGTH)[0]\n",
    "text_output = tokenizer.decode(token_ids_output, skip_special_tokens=True)\n",
    "print(text_output)\n",
    "\n",
    "embeddings_output = model(\n",
    "    tokenizer.encode(\n",
    "        text_output, padding=\"max_length\", max_length=128, return_tensors=\"pt\"\n",
    "    )\n",
    "    .to(device)\n",
    "    .reshape(1, -1)\n",
    ").encoder_last_hidden_state\n",
    "embeddings_output = torch.mean(embeddings_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(embedding_label, embeddings_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = 50\n",
    "results = hits(model, tokenizer, valid_ds[:validation_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = tokenizer.decode(valid_ds[0][\"input_ids\"], skip_special_tokens=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Hits Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def training_data_to_text(input_ids, tokenizer=tokenizer):\n",
    "    text = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "    # 3 -> len(<s>), 7 -> len(<mask>.)\n",
    "\n",
    "    return text[3 : text.find(tokenizer.mask_token) + 7]\n",
    "\n",
    "\n",
    "def generate_beam_search(model, tokenizer, text, beam_size=5, max_length=128):\n",
    "    # Text Example: My name is <mask>.\n",
    "    input_ids = tokenizer.encode(\n",
    "        text, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    beam_outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=beam_size,\n",
    "        num_return_sequences=beam_size,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        tokenizer.decode(beam_output, skip_special_tokens=True)\n",
    "        for beam_output in beam_outputs\n",
    "    ]\n",
    "\n",
    "\n",
    "def compute_hits(model, tokenizer, valid_ds, beam_size=5, max_length=128, debug=False):\n",
    "    results = []\n",
    "    hits = 0\n",
    "    data_size = valid_ds[\"input_ids\"].shape[0]\n",
    "\n",
    "    for i in tqdm(range(data_size)):\n",
    "        text = training_data_to_text(valid_ds[\"input_ids\"][i], tokenizer=tokenizer)\n",
    "        label = tokenizer.decode(valid_ds[\"labels\"][i], skip_special_tokens=True)\n",
    "        output_list = generate_beam_search(\n",
    "            text=text,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            beam_size=beam_size,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        if label.lower() in list(map(lambda x: x.lower(), output_list)):\n",
    "            hits += 1\n",
    "\n",
    "            if debug:\n",
    "                print(\"Text: \", text)\n",
    "                print(\"Label: \", label)\n",
    "                print(\"Output: \", output_list)\n",
    "                print()\n",
    "\n",
    "            results.append((text, label, output_list, True))\n",
    "        else:\n",
    "            results.append((text, label, output_list, False))\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(results, columns=[\"text\", \"label\", \"output_list\", \"match\"]),\n",
    "        hits,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66899476807443989ad5510028de337a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  133\n"
     ]
    }
   ],
   "source": [
    "# Results beam size 1\n",
    "batch = 1000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=1, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>output_list</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>teacher has people with this profession of Donald McAlpine. screenwriter has people with this profession of Peter Coyote. composer has people with this profession of&lt;mask&gt;.</td>\n",
       "      <td>Bill Wyman</td>\n",
       "      <td>[John Cage]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Scream 2 has music of Marco Beltrami. Red Dragon has music of Danny Elfman. The Da Vinci Code has music of&lt;mask&gt;.</td>\n",
       "      <td>Hans Zimmer</td>\n",
       "      <td>[John Williams]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>16th United States Congress has legislative sessions of 13th United States Congress. 95th United States Congress has legislative sessions of 110th United States Congress. New Jersey has legislative sessions of&lt;mask&gt;.</td>\n",
       "      <td>99th United States Congress</td>\n",
       "      <td>[16th United States Congress]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>56th Golden Globe Awards has honored for of Elizabeth. George Lucas has honored for of American Graffiti. Dakota Fanning has honored for of&lt;mask&gt;.</td>\n",
       "      <td>War of the Worlds</td>\n",
       "      <td>[The Twilight Saga: Eclipse]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Mortal Kombat: Annihilation has actor of James Remar. Lethal Weapon 4 has actor of Mel Gibson. Hercules has actor of&lt;mask&gt;.</td>\n",
       "      <td>Keith David</td>\n",
       "      <td>[John Cleese]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Star Wars: The Clone Wars has genre of adventure film. A Christmas Carol has genre of fantasy. Joseph Haydn has genre of&lt;mask&gt;.</td>\n",
       "      <td>chamber music</td>\n",
       "      <td>[opera]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>Mortal Kombat: Annihilation has actor of James Remar. Lethal Weapon 4 has actor of Mel Gibson. L.A. Confidential has actor of&lt;mask&gt;.</td>\n",
       "      <td>Kevin Spacey</td>\n",
       "      <td>[Michael Caine]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Henry King has cause of death of myocardial infarction. Susan Hayward has cause of death of brain tumor. Edward G. Robinson has cause of death of&lt;mask&gt;.</td>\n",
       "      <td>cancer</td>\n",
       "      <td>[myocardial infarction]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Star Wars: The Clone Wars has genre of adventure film. A Christmas Carol has genre of fantasy. Deep Purple has genre of&lt;mask&gt;.</td>\n",
       "      <td>psychedelic rock</td>\n",
       "      <td>[rock music]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>BAFTA Award for Best Editing has nominated for of Alien. MTV Movie Award for Best Kiss has nominated for of Monster. Golden Raspberry Award for Worst Prequel, Remake, Rip-off or Sequel has nominated for of&lt;mask&gt;.</td>\n",
       "      <td>Sex and the City 2</td>\n",
       "      <td>[The Adventures of Ford Fairlane]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                         text                        label                        output_list  match\n",
       "983                                              teacher has people with this profession of Donald McAlpine. screenwriter has people with this profession of Peter Coyote. composer has people with this profession of<mask>.                   Bill Wyman                        [John Cage]  False\n",
       "496                                                                                                         Scream 2 has music of Marco Beltrami. Red Dragon has music of Danny Elfman. The Da Vinci Code has music of<mask>.                  Hans Zimmer                    [John Williams]  False\n",
       "142  16th United States Congress has legislative sessions of 13th United States Congress. 95th United States Congress has legislative sessions of 110th United States Congress. New Jersey has legislative sessions of<mask>.  99th United States Congress      [16th United States Congress]  False\n",
       "457                                                                        56th Golden Globe Awards has honored for of Elizabeth. George Lucas has honored for of American Graffiti. Dakota Fanning has honored for of<mask>.            War of the Worlds       [The Twilight Saga: Eclipse]  False\n",
       "139                                                                                               Mortal Kombat: Annihilation has actor of James Remar. Lethal Weapon 4 has actor of Mel Gibson. Hercules has actor of<mask>.                  Keith David                      [John Cleese]  False\n",
       "238                                                                                           Star Wars: The Clone Wars has genre of adventure film. A Christmas Carol has genre of fantasy. Joseph Haydn has genre of<mask>.                chamber music                            [opera]  False\n",
       "634                                                                                      Mortal Kombat: Annihilation has actor of James Remar. Lethal Weapon 4 has actor of Mel Gibson. L.A. Confidential has actor of<mask>.                 Kevin Spacey                    [Michael Caine]  False\n",
       "82                                                                   Henry King has cause of death of myocardial infarction. Susan Hayward has cause of death of brain tumor. Edward G. Robinson has cause of death of<mask>.                       cancer            [myocardial infarction]  False\n",
       "860                                                                                            Star Wars: The Clone Wars has genre of adventure film. A Christmas Carol has genre of fantasy. Deep Purple has genre of<mask>.             psychedelic rock                       [rock music]  False\n",
       "417      BAFTA Award for Best Editing has nominated for of Alien. MTV Movie Award for Best Kiss has nominated for of Monster. Golden Raspberry Award for Worst Prequel, Remake, Rip-off or Sequel has nominated for of<mask>.           Sex and the City 2  [The Adventures of Ford Fairlane]  False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results[\"match\"] == False].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ad16a6fe7645eea5bd452321cde812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  3380\n"
     ]
    }
   ],
   "source": [
    "# Results beam size 3\n",
    "batch = 20000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=3, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389ed18cfaa9425fb0a1cf8508b474f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  5338\n"
     ]
    }
   ],
   "source": [
    "# Results beam size 10\n",
    "batch = 20000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=10, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5029cfb37363452b922c7b5aeb0e7852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  2600\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7761f2d79d164b6db98a14ded10461a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  3945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca02d3c1a25f45a5a5afc8227e7857d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits:  6306\n"
     ]
    }
   ],
   "source": [
    "# Results beam size 1\n",
    "batch = 20000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=1, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)\n",
    "\n",
    "# Results beam size 3\n",
    "batch = 20000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=3, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)\n",
    "\n",
    "# Results beam size 10\n",
    "batch = 20000  # Max -> len(valid_ds)\n",
    "results, hits = compute_hits(\n",
    "    model, tokenizer, valid_ds[:batch], beam_size=10, debug=False\n",
    ")\n",
    "\n",
    "print(\"Hits: \", hits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paulo',\n",
       " 'Francisco',\n",
       " 'Jo',\n",
       " 'Tom',\n",
       " 'José',\n",
       " 'Pedro',\n",
       " 'Jose',\n",
       " 'Miguel',\n",
       " 'Antonio',\n",
       " 'Juan']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "TXT = \"Brazil capital is São <mask>.\"\n",
    "\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(input_ids.to(device)).logits\n",
    "\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(10)\n",
    "\n",
    "tokenizer.decode(predictions).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['€', '$', '£', 'A', '5']\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "TXT = \"New Zeland has capital of <mask>.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "logits = model(input_ids.to(device)).logits\n",
    "\n",
    "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
    "probs = logits[0, masked_index].softmax(dim=0)\n",
    "values, predictions = probs.topk(5)\n",
    "\n",
    "predicted_tokens = tokenizer.batch_decode(predictions)\n",
    "\n",
    "suggested_words = tokenizer.decode(predictions).split()\n",
    "\n",
    "# Gerar 5 preenchimentos de 2 tokens cada\n",
    "combinations = [\n",
    "    (suggested_words[i], suggested_words[j]) for i in range(5) for j in range(i + 1, 5)\n",
    "]\n",
    "\n",
    "print(suggested_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate(txt):\n",
    "    # Definir hiperparâmetros da busca beam search\n",
    "    num_beams = 10\n",
    "    num_tokens = 10\n",
    "\n",
    "    # Tokenizar a entrada\n",
    "    input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "    # Gerar preenchimentos usando busca beam search\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=input_ids.shape[-1] + num_tokens,\n",
    "        num_beams=num_beams,\n",
    "        num_return_sequences=num_beams,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "    # Decodificar as sequências geradas\n",
    "    decoded_outputs = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # Gerar combinações de 2 tokens para as sequências decodificadas\n",
    "    print(decoded_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepts - Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English', 'Tamil', 'Urdu', 'French', 'Telugu', 'Arabic', 'Spanish', 'Italian', 'English literature', 'Malay']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Brasil has official language of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spanish', 'English', 'portuguese', 'Latin', 'Brazilian', 'French', 'Italian', 'Brasilian', 'Bolero', 'Arabic']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Spain has the official language of Spanish. EUA has official language of English. Brasil has official language of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concets - Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer animation', 'action film', 'drama film', 'adventure film', 'fantasy', 'video game', 'vampire', 'anime', 'video game music', 'video game game']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Mortal Kombat has type of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moscow', 'Saint Petersburg', 'Kiev', 'St. Petersburg', 'Tbilisi', 'Minsk', 'Alexandria', 'Baku', 'Rome', 'Varna']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Russia has capital of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Porto', 'Portugal City', 'Rio de Janeiro', 'Lisbon', 'Puerto Rico', 'Sarasota', 'Porto City', 'Amsterdam', 'Madrid', 'Portugal']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Portugal has capital of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jenna Bush Hager', 'Loretta Devine', 'Geraldine Chaplin', 'Katharine Hepburn', 'Geraldine Somerville', 'Jill Clayburgh', 'Jennifer Aniston', 'Geraldine Page', 'Loretta Young', 'Hillary Clinton']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Barack Obama is married of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama', 'Barbara Walters', 'Barbara Hershey', 'Debra Winger', 'Michelle Obama', 'Lauren Bacall', 'Lauren Holly', 'Michelle Forbes', 'Michelle Branch', 'Lauren Conrad']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"John Kennedy is married of Jacqueline Kennedy. George Bush is married of Laura Bush. B. Obama is married of <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wyatt Earp', 'The Godfather Part III', 'Thelma & Louise', 'The Godfather Part II', 'The Adventures of Ford Fairlane', 'Saving Private Ryan', 'The Adventures of Tintin', 'The Adventures of Pluto Nash', 'Sideways', 'The Quiet American']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Films: <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Wars Episode IV: A New Hope', 'Star Trek', 'Star Trek: First Contact', 'Star Trek IV: The Voyage Home', 'Star Trek VI: The Undiscovered Country', 'Star Trek V: The Final Frontier', 'Star Trek: The Original Series', 'Star Trek: The Next Generation', 'Star Trek: Nemesis', 'Star Wars']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Films: Star Wars, Star Trek, <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer hardware', 'computer science', 'computer animation', 'electronic keyboard', 'artificial intelligence', 'software engineering', 'acoustic guitar', 'programming', 'computer engineering', 'software']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Computer <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack Obama', 'George W. Bush', 'Theodore Bikel', 'Sidney Sheldon', 'Barry Gibb', 'George Lucas', 'Barry Pepper', 'Theodore Roosevelt', 'Michael Moore', 'George Harrison']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Barack <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Walt Disney Company', 'Walt Disney Pictures', 'Walt Disney', 'Pixar', 'George Lucas', 'DreamWorks Animation', 'Hollywood Squares', 'Hollywood Pictures', 'Hollywood Forever Cemetery', 'DreamWorks']\n"
     ]
    }
   ],
   "source": [
    "TXT = \"Walt Disney <mask>.\"\n",
    "\n",
    "_generate(TXT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
