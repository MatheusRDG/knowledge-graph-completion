{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques to convert KGC in plain text\n",
    "\n",
    "We will follow [From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer](https://arxiv.org/pdf/2202.02113.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from src.utils import load_fb15k237, load_wn18rr, get_hist\n",
    "\n",
    "PATH_FB15k237 = \"data/datasets_knowledge_embedding/FB15k-237\"\n",
    "PATH_WN18RR = \"data/datasets_knowledge_embedding/WN18RR/text\"\n",
    "PATH_FB15k = \"data/datasets_knowledge_embedding/FB15k\"\n",
    "PATH_WN18 = \"data/datasets_knowledge_embedding/WN18/text\"\n",
    "\n",
    "train_fb15k, valid_fb15k, test_fb15k, _ = load_fb15k237(PATH_FB15k)\n",
    "\n",
    "train_fb15k237, valid_fb15k237, test_fb15k237, entity2wikidata = load_fb15k237(\n",
    "    PATH_FB15k237\n",
    ")\n",
    "\n",
    "train_wn18, valid_wn18, test_wn18 = load_wn18rr(PATH_WN18)\n",
    "\n",
    "train_wn18rr, valid_wn18rr, test_wn18rr = load_wn18rr(PATH_WN18RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data without entity description\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# FB15k\n",
    "all_data_fb15k = pd.concat([train_fb15k, valid_fb15k, test_fb15k], axis=0)\n",
    "\n",
    "df_entity = pd.DataFrame(entity2wikidata.keys(), columns=[\"head\"])\n",
    "\n",
    "all_data_fb15k = all_data_fb15k[\n",
    "    all_data_fb15k[\"head\"].isin(df_entity[\"head\"])\n",
    "    & all_data_fb15k[\"tail\"].isin(df_entity[\"head\"])\n",
    "]\n",
    "\n",
    "# FB15k-237\n",
    "all_data_fb15k237 = pd.concat([train_fb15k237, valid_fb15k237, test_fb15k237], axis=0)\n",
    "\n",
    "all_data_fb15k237 = all_data_fb15k237[\n",
    "    all_data_fb15k237[\"head\"].isin(df_entity[\"head\"])\n",
    "    & all_data_fb15k237[\"tail\"].isin(df_entity[\"head\"])\n",
    "]\n",
    "\n",
    "# WN18\n",
    "all_data_wn18 = pd.concat([train_wn18, valid_wn18, test_wn18], axis=0)\n",
    "\n",
    "# WN18RR\n",
    "all_data_wn18rr = pd.concat([train_wn18rr, valid_wn18rr, test_wn18rr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_relation_to_text(relation):\n",
    "    return \"has \" + relation.split(\"/\")[-1].replace(\"_\", \" \") + \" of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB15k\n",
    "all_data_fb15k[\"head_text\"] = all_data_fb15k[\"head\"].apply(\n",
    "    lambda i: entity2wikidata[i][\"label\"]\n",
    ")\n",
    "all_data_fb15k[\"relation_text\"] = all_data_fb15k[\"relation\"].apply(\n",
    "    lambda i: map_relation_to_text(i)\n",
    ")\n",
    "all_data_fb15k[\"tail_text\"] = all_data_fb15k[\"tail\"].apply(\n",
    "    lambda i: entity2wikidata[i][\"label\"]\n",
    ")\n",
    "\n",
    "all_data_fb15k[\"text\"] = (\n",
    "    all_data_fb15k[\"head_text\"]\n",
    "    + \" \"\n",
    "    + all_data_fb15k[\"relation_text\"]\n",
    "    + \" \"\n",
    "    + all_data_fb15k[\"tail_text\"]\n",
    "    + \".\"\n",
    ")\n",
    "\n",
    "# FB15k-237\n",
    "all_data_fb15k237[\"head_text\"] = all_data_fb15k237[\"head\"].apply(\n",
    "    lambda i: entity2wikidata[i][\"label\"]\n",
    ")\n",
    "all_data_fb15k237[\"relation_text\"] = all_data_fb15k237[\"relation\"].apply(\n",
    "    lambda i: map_relation_to_text(i)\n",
    ")\n",
    "all_data_fb15k237[\"tail_text\"] = all_data_fb15k237[\"tail\"].apply(\n",
    "    lambda i: entity2wikidata[i][\"label\"]\n",
    ")\n",
    "\n",
    "all_data_fb15k237[\"text\"] = (\n",
    "    all_data_fb15k237[\"head_text\"]\n",
    "    + \" \"\n",
    "    + all_data_fb15k237[\"relation_text\"]\n",
    "    + \" \"\n",
    "    + all_data_fb15k237[\"tail_text\"]\n",
    "    + \".\"\n",
    ")\n",
    "\n",
    "# WN18\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "all_data_wn18[\"head_text\"] = all_data_wn18[\"head\"].apply(\n",
    "    lambda i: \" \".join(wn.synset(i).lemmas()[0].name().split(\"_\"))\n",
    ")\n",
    "\n",
    "all_data_wn18[\"relation_text\"] = all_data_wn18[\"relation\"].apply(\n",
    "    lambda i: map_relation_to_text(\" \".join(i.split(\"_\")))\n",
    ")\n",
    "\n",
    "all_data_wn18[\"tail_text\"] = all_data_wn18[\"tail\"].apply(\n",
    "    lambda i: \" \".join(wn.synset(i).lemmas()[0].name().split(\"_\"))\n",
    ")\n",
    "\n",
    "# WN18RR\n",
    "\n",
    "all_data_wn18rr[\"head_text\"] = all_data_wn18rr[\"head\"].apply(\n",
    "    lambda i: \" \".join(wn.synset(i).lemmas()[0].name().split(\"_\"))\n",
    ")\n",
    "\n",
    "all_data_wn18rr[\"relation_text\"] = all_data_wn18rr[\"relation\"].apply(\n",
    "    lambda i: map_relation_to_text(\" \".join(i.split(\"_\")))\n",
    ")\n",
    "\n",
    "all_data_wn18rr[\"tail_text\"] = all_data_wn18rr[\"tail\"].apply(\n",
    "    lambda i: \" \".join(wn.synset(i).lemmas()[0].name().split(\"_\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat(\n",
    "    [all_data_fb15k, all_data_fb15k237, all_data_wn18, all_data_wn18rr], axis=0\n",
    ")\n",
    "all_data = all_data[[\"head_text\", \"relation_text\", \"tail_text\", \"text\"]]\n",
    "\n",
    "all_data.drop_duplicates(inplace=True)\n",
    "\n",
    "all_data[\"id\"] = all_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def train_demonstration_generator(row):\n",
    "    # select 2 samples of rows with same relation.\n",
    "    # generate triple of select rows\n",
    "    # contat triples and return\n",
    "\n",
    "    # other heuristic is split all relations e groups of 3 and use 2 for demonstration 1 to fill\n",
    "\n",
    "    to_fill = row.head_text + \" \" + row.relation_text + \" \"\n",
    "\n",
    "    try:\n",
    "        return \" \".join(\n",
    "            all_data[\n",
    "                (all_data[\"relation_text\"] == row.relation_text)\n",
    "                & (all_data[\"id\"] != row.id)\n",
    "            ]\n",
    "            .sample(2, random_state=42)[\"text\"]\n",
    "            .to_list()\n",
    "            + [to_fill]\n",
    "        )\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import swifter\n",
    "\n",
    "all_data[\"demonstration_input\"] = all_data.swifter.apply(\n",
    "    lambda row: train_demonstration_generator(row), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/data_processed/FB15k_FB15k237_WN18_WN18RR/\", exist_ok=True)\n",
    "\n",
    "all_data.to_csv(\n",
    "    \"data/data_processed/FB15k_FB15k237_WN18_WN18RR/processed_data.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
