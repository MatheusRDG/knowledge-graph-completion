{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bart Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\", forced_bos_token_id=0)\n",
    "# tok = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# example_english_phrase = \"Dominican Republic has form of government of <mask>\"\n",
    "# batch = tok(example_english_phrase, return_tensors=\"pt\")\n",
    "# generated_ids = model.generate(batch[\"input_ids\"])\n",
    "# tok.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import load_fb15k237\n",
    "\n",
    "PATH_FB15k237 = \"data/datasets_knowledge_embedding/FB15k-237\"\n",
    "\n",
    "train, valid, test, entity2wikidata = load_fb15k237(PATH_FB15k237)\n",
    "processed_data = pd.read_csv(PATH_FB15k237 + \"/processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    BartConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_LENGHT = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Select model\n",
    "# MODEL = \"facebook/bart-large\"\n",
    "# MODEL = \"facebook/bart-base\"\n",
    "MODEL = \"lucadiliello/bart-small\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL).to(device)\n",
    "tok = BartTokenizer.from_pretrained(MODEL, model_max_length=MAX_LENGHT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = processed_data.iloc[5]\n",
    "\n",
    "# text_sample = sample.demonstration_input + sample.tail_text + \".\"\n",
    "# print(text_sample)\n",
    "\n",
    "# to_mask = sample.tail_text\n",
    "# print(to_mask)\n",
    "\n",
    "# ids_to_mask = tok.convert_tokens_to_ids(to_mask)\n",
    "# print(ids_to_mask)\n",
    "# print(tok.convert_ids_to_tokens(ids_to_mask))\n",
    "\n",
    "# print(\n",
    "#     tok.encode(\n",
    "#         \"i want to mask Hello world\",\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=MAX_LENGHT,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )[:10]\n",
    "# )\n",
    "\n",
    "# print(\n",
    "#     tok.encode(\n",
    "#         \"i want to mask <mask>\",\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=MAX_LENGHT,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#     )[:10]\n",
    "# )\n",
    "\n",
    "# print(tok.all_special_tokens)\n",
    "# print(tok.all_special_ids)\n",
    "\n",
    "# tok.mask_token_id#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[\"data_input\"] = (\n",
    "    processed_data[\"demonstration_input\"] + \"%s.\" % tok.mask_token\n",
    ")\n",
    "processed_data[\"data_label\"] = (\n",
    "    processed_data[\"demonstration_input\"] + processed_data[\"tail_text\"] + \".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = True\n",
    "\n",
    "if dev:\n",
    "    processed_data = processed_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Codifica as strings de entrada e rótulos como sequências de tokens BART\n",
    "encoded_input = tok(\n",
    "    list(processed_data[\"data_input\"]),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGHT,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    ")\n",
    "encoded_label = tok(\n",
    "    list(processed_data[\"data_label\"]),\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=MAX_LENGHT,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    ")\n",
    "\n",
    "# Cria uma lista de exemplos\n",
    "examples = []\n",
    "for i in range(len(processed_data)):\n",
    "    input_ids = encoded_input[\"input_ids\"][i]\n",
    "    labels = encoded_label[\"input_ids\"][i]\n",
    "    examples.append({\"input_ids\": input_ids, \"labels\": labels})\n",
    "\n",
    "# Cria um objeto DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "\n",
    "# Prepara os dados de treinamento\n",
    "prepared_data = data_collator(examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "class DatasetKGC(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data[\"input_ids\"] = self.data[\"input_ids\"].to(device)\n",
    "        self.data[\"labels\"] = self.data[\"labels\"].to(device)\n",
    "        self.num_rows = self.data[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _input = self.data[\"input_ids\"][idx].squeeze(0)\n",
    "        label = self.data[\"labels\"][idx].squeeze(0)\n",
    "\n",
    "        return (_input, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetKGC = DatasetKGC(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(DatasetKGC(prepared_data), batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch shape\n",
    "train_features, train_labels = next(iter(data_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "epochs = 5\n",
    "loss_epoch = []\n",
    "lr = 1e-3\n",
    "cross = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "pbar = tqdm(range(epochs), desc=\"Epochs\")\n",
    "\n",
    "\n",
    "for epoch in pbar:\n",
    "    epoch_loss = 0\n",
    "    for _input, label in data_loader:\n",
    "\n",
    "        pbar.set_description(\"Epoch %s\" % epoch)\n",
    "        pbar.refresh()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        _dt = model(_input, return_dict=True)\n",
    "        logits = _dt.logits\n",
    "        loss = cross(logits.view(-1, logits.size(-1)), label.view(-1))\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "    torch.cuda.empty_cache()\n",
    "    loss_epoch.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Transformer Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(loss_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
