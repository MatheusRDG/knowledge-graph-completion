{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.utils import load_fb15k237\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "PATH_FB15k237 = \"data/datasets_knowledge_embedding/FB15k-237\"\n",
    "\n",
    "train, valid, test, entity2wikidata = load_fb15k237(PATH_FB15k237)\n",
    "processed_data = pd.read_csv(PATH_FB15k237 + \"/processed_data_v2.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Select model\n",
    "# MODEL = \"facebook/bart-large\"\n",
    "# MODEL = \"facebook/bart-base\"\n",
    "MODEL = \"lucadiliello/bart-small\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL).cuda().float().to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "DEV_BATCH = 100\n",
    "MAX_LENGTH = 50  # model.config.d_model\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "dev = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[\"data_input\"] = (\n",
    "    processed_data[\"demonstration_input\"] + \"%s.\" % tokenizer.mask_token\n",
    ")\n",
    "processed_data[\"data_label\"] = (\n",
    "    processed_data[\"demonstration_input\"] + processed_data[\"tail_text\"] + \".\"\n",
    ")\n",
    "\n",
    "if dev:\n",
    "    if DEV_BATCH == -1:\n",
    "        pass\n",
    "    else:\n",
    "        processed_data = processed_data.head(DEV_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import copy\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "class DatasetKGC(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data[\"input_ids\"] = self.data[\"input_ids\"]\n",
    "        self.data[\"labels\"] = self.data[\"labels\"]\n",
    "        self.num_rows = self.data[\"input_ids\"].shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _input = self.data[\"input_ids\"][idx].squeeze(0)\n",
    "        label = self.data[\"labels\"][idx].squeeze(0)\n",
    "\n",
    "        return {\"input_ids\": _input, \"labels\": label}\n",
    "\n",
    "\n",
    "def encode_data(data):\n",
    "    # Codifica as strings de entrada e rótulos como sequências de tokens BART\n",
    "    encoded_input = tokenizer(\n",
    "        list(data[\"data_input\"]),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    encoded_label = tokenizer(\n",
    "        list(data[\"data_label\"]),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # Cria uma lista de exemplos\n",
    "    examples = []\n",
    "    for i in range(len(data)):\n",
    "        input_ids = encoded_input[\"input_ids\"][i]\n",
    "        labels = encoded_label[\"input_ids\"][i]\n",
    "        examples.append({\"input_ids\": input_ids, \"labels\": labels})\n",
    "\n",
    "    # Cria um objeto DataCollatorForLanguageModeling\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "    # Prepara os dados de treinamento\n",
    "    prepared_data = data_collator(examples)\n",
    "\n",
    "    return prepared_data\n",
    "\n",
    "\n",
    "def train_valid_split(data):\n",
    "    train, valid = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    return encode_data(train), encode_data(valid)\n",
    "\n",
    "\n",
    "def generate_train_valid_dataloader(data):\n",
    "    train, valid = train_valid_split(data)\n",
    "\n",
    "    train_loader = DataLoader(DatasetKGC(train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    valid_loader = DataLoader(DatasetKGC(valid), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def generate_train_valid_dataset(data):\n",
    "    train, valid = train_valid_split(data)\n",
    "\n",
    "    train_loader = DatasetKGC(train)\n",
    "\n",
    "    valid_loader = DatasetKGC(valid)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "def _decode(tokens, tokenizer=tokenizer, batch=True):\n",
    "    # Remove padding tokens and decode\n",
    "    # tokens = tokens[tokens != -100]\n",
    "    if batch:\n",
    "        return tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def _decode_error(tokens_i, tokens_j, tokenizer=tokenizer):\n",
    "    \"\"\" \"\"\"\n",
    "    # Remove padding tokens\n",
    "    text_i = _decode(tokens_i)\n",
    "    text_j = _decode(tokens_j)\n",
    "\n",
    "    # print(text_i)\n",
    "    # print(text_j)\n",
    "\n",
    "    error = 0\n",
    "\n",
    "    len_i = len(tokens_i)\n",
    "    len_j = len(tokens_j)\n",
    "    total_tokens = max(len_i, len_j)\n",
    "\n",
    "    diff_shape = abs(len_i - len_j)\n",
    "\n",
    "    error += diff_shape\n",
    "\n",
    "    for i in range(min(len_i, len_j)):\n",
    "        if text_i[i] != text_j[i]:\n",
    "            error += 1\n",
    "\n",
    "    return (error, total_tokens, error / total_tokens)\n",
    "\n",
    "\n",
    "def _decode_error_epoch(model, valid_dataset, debug=False):\n",
    "    error_tokens = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _input, label in tqdm(valid_dataset, desc=\"Analyzing decoder error\"):\n",
    "            model_output = model.generate(_input.to(device), max_length=MAX_LENGTH)\n",
    "\n",
    "            for i in range(len(model_output)):\n",
    "                a, b, percentage = _decode_error(label[i], model_output[i].to(\"cpu\"))\n",
    "                error_tokens += a\n",
    "                total_tokens += b\n",
    "\n",
    "        if debug:\n",
    "            print(\"Total tokens analyzed: %d\" % total_tokens)\n",
    "            print(\"Total erroneous tokens predicted: %d\" % error_tokens)\n",
    "            print(\"Percentage of error: %.3f%%\" % ((error_tokens / total_tokens) * 100))\n",
    "\n",
    "        return error_tokens, total_tokens, error_tokens / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipeline\n",
    "# train_loader, valid_loader = generate_train_valid_dataloader(processed_data)\n",
    "\n",
    "# Overfitting test\n",
    "# train_loader = DataLoader(\n",
    "#     DatasetKGC(encode_data(processed_data)), batch_size=BATCH_SIZE, shuffle=False\n",
    "# )\n",
    "# valid_loader = train_loader\n",
    "\n",
    "train_ds, valid_ds = generate_train_valid_dataset(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a small error because the mask token is not filled at this point.\n",
    "# sample_feature = valid_features[0]\n",
    "# sample_label = valid_labels[0]\n",
    "# sample_output = model.generate(\n",
    "#     sample_feature.reshape(1, -1).to(device), max_length=MAX_LENGTH\n",
    "# )\n",
    "\n",
    "# print(_decode(sample_feature, batch=False))\n",
    "# print(_decode(sample_label, batch=False))\n",
    "# print(_decode(sample_output[0], batch=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "epochs = 60\n",
    "epoch_accuracy_frequency = 30  # if epoch % accuracy -> compute\n",
    "loss_epoch = []\n",
    "lr = 1e-5\n",
    "cross = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# pbar = tqdm(range(1, epochs + 1), desc=\"Epochs\")\n",
    "# decoder_error = []\n",
    "\n",
    "# decoder_error.append(_decode_error_epoch(model, valid_loader, debug=True))\n",
    "\n",
    "# for epoch in pbar:\n",
    "#     epoch_loss = 0\n",
    "\n",
    "#     pbar.set_description(\"Epoch %s\" % epoch)\n",
    "#     pbar.refresh()\n",
    "\n",
    "#     for _input, label in train_loader:\n",
    "#         model.zero_grad()\n",
    "\n",
    "#         _dt = model(_input.to(device), labels=label.to(device), return_dict=True)\n",
    "\n",
    "#         # _dt_label = model(label.to(device), return_dict=True)\n",
    "\n",
    "#         # logits_input = _dt.logits\n",
    "#         # logits_label = _dt_label.logits\n",
    "\n",
    "#         # loss = cross(\n",
    "#         #     logits_input.view(-1, logits_input.size(-1)).softmax(dim=-1),\n",
    "#         #     logits_label.view(-1, logits_input.size(-1)),\n",
    "#         # )\n",
    "\n",
    "#         loss = _dt.loss\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     if epoch % epoch_accuracy_frequency == 0:\n",
    "#         decoder_error.append(_decode_error_epoch(model, valid_loader, debug=True))\n",
    "\n",
    "#     pbar.set_postfix(loss=epoch_loss)\n",
    "#     loss_epoch.append(epoch_loss)\n",
    "\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    global loss_values\n",
    "    loss = float(eval_pred[\"loss\"])\n",
    "    loss_values.append(eval_pred)\n",
    "\n",
    "    return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "params = {\n",
    "    # Dir\n",
    "    \"output_dir\": \"model/model_bart\",\n",
    "    # Batch\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"per_device_eval_batch_size\": 2,\n",
    "    # Learning rate\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"seed\": 42,\n",
    "    # Epochs\n",
    "    \"num_train_epochs\": 50,\n",
    "    # Logging\n",
    "    \"logging_dir\": \"model/logs\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 10,\n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"eval_steps\": 10,\n",
    "    # Checkpoint\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 10,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"report_to\": \"tensorboard\",\n",
    "    \"ddp_find_unused_parameters\": False,\n",
    "    \"warmup_steps\": 2,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Dir\n",
    "    output_dir=params[\"output_dir\"],\n",
    "    # Batch\n",
    "    per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "    # Learning Rate\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    seed=params[\"seed\"],\n",
    "    # Epoch\n",
    "    num_train_epochs=params[\"num_train_epochs\"],\n",
    "    # logging\n",
    "    logging_dir=params[\"logging_dir\"],\n",
    "    logging_strategy=params[\"logging_strategy\"],\n",
    "    logging_steps=params[\"logging_steps\"],\n",
    "    # Evaluation\n",
    "    # evaluation_strategy=params[\"evaluation_strategy\"],\n",
    "    # eval_steps=params[\"eval_steps\"],\n",
    "    # Checkpoint\n",
    "    save_strategy=params[\"save_strategy\"],\n",
    "    save_steps=params[\"save_steps\"],\n",
    "    save_total_limit=params[\"save_total_limit\"],\n",
    "    # pretraining\n",
    "    ddp_find_unused_parameters=params[\"ddp_find_unused_parameters\"],\n",
    "    warmup_steps=params[\"warmup_steps\"],\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    # test\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model/model_bart\\checkpoint-200\n",
      "Configuration saved in model/model_bart\\checkpoint-200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0035, 'learning_rate': 4.507007007007007e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-200\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-120] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-240\n",
      "Configuration saved in model/model_bart\\checkpoint-240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0028, 'learning_rate': 4.406906906906907e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-240\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-240\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-240\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-160] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-280\n",
      "Configuration saved in model/model_bart\\checkpoint-280\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0044, 'learning_rate': 4.306806806806807e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-280\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-280\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-280\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-200] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-320\n",
      "Configuration saved in model/model_bart\\checkpoint-320\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'learning_rate': 4.2067067067067065e-05, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-320\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-320\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-320\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-240] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-360\n",
      "Configuration saved in model/model_bart\\checkpoint-360\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 4.1066066066066066e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-360\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-360\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-360\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-280] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-400\n",
      "Configuration saved in model/model_bart\\checkpoint-400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0026, 'learning_rate': 4.006506506506507e-05, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-400\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-320] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-440\n",
      "Configuration saved in model/model_bart\\checkpoint-440\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0028, 'learning_rate': 3.906406406406406e-05, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-440\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-440\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-440\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-360] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-480\n",
      "Configuration saved in model/model_bart\\checkpoint-480\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 3.8063063063063064e-05, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-480\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-480\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-480\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-520\n",
      "Configuration saved in model/model_bart\\checkpoint-520\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0015, 'learning_rate': 3.7062062062062065e-05, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-520\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-520\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-520\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-440] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-560\n",
      "Configuration saved in model/model_bart\\checkpoint-560\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0024, 'learning_rate': 3.6061061061061066e-05, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-560\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-560\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-560\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-480] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-600\n",
      "Configuration saved in model/model_bart\\checkpoint-600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 3.506006006006006e-05, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-600\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-520] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-640\n",
      "Configuration saved in model/model_bart\\checkpoint-640\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0016, 'learning_rate': 3.405905905905906e-05, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-640\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-640\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-640\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-560] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-680\n",
      "Configuration saved in model/model_bart\\checkpoint-680\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 3.3058058058058064e-05, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-680\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-680\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-680\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-600] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-720\n",
      "Configuration saved in model/model_bart\\checkpoint-720\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 3.205705705705706e-05, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-720\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-720\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-720\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-640] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-760\n",
      "Configuration saved in model/model_bart\\checkpoint-760\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0018, 'learning_rate': 3.105605605605606e-05, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-760\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-760\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-760\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-680] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-800\n",
      "Configuration saved in model/model_bart\\checkpoint-800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 3.0055055055055058e-05, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-800\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-720] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-840\n",
      "Configuration saved in model/model_bart\\checkpoint-840\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0012, 'learning_rate': 2.9054054054054052e-05, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-840\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-840\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-840\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-760] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-880\n",
      "Configuration saved in model/model_bart\\checkpoint-880\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'learning_rate': 2.8053053053053054e-05, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-880\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-880\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-880\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-800] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-920\n",
      "Configuration saved in model/model_bart\\checkpoint-920\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 2.7052052052052052e-05, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-920\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-920\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-920\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-840] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-960\n",
      "Configuration saved in model/model_bart\\checkpoint-960\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.605105105105105e-05, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-960\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-960\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-960\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-880] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1000\n",
      "Configuration saved in model/model_bart\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.505005005005005e-05, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-920] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1040\n",
      "Configuration saved in model/model_bart\\checkpoint-1040\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 2.404904904904905e-05, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1040\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1040\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1040\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-960] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1080\n",
      "Configuration saved in model/model_bart\\checkpoint-1080\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 2.3048048048048047e-05, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1080\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1080\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1080\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1120\n",
      "Configuration saved in model/model_bart\\checkpoint-1120\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0009, 'learning_rate': 2.204704704704705e-05, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1120\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1120\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1120\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1040] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1160\n",
      "Configuration saved in model/model_bart\\checkpoint-1160\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 2.1046046046046047e-05, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1160\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1160\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1160\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1080] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1200\n",
      "Configuration saved in model/model_bart\\checkpoint-1200\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'learning_rate': 2.0045045045045048e-05, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1200\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1200\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1200\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1120] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1240\n",
      "Configuration saved in model/model_bart\\checkpoint-1240\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 1.9044044044044046e-05, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1240\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1240\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1240\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1160] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1280\n",
      "Configuration saved in model/model_bart\\checkpoint-1280\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.8043043043043044e-05, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1280\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1280\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1280\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1200] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1320\n",
      "Configuration saved in model/model_bart\\checkpoint-1320\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.7042042042042042e-05, 'epoch': 33.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1320\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1320\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1320\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1240] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1360\n",
      "Configuration saved in model/model_bart\\checkpoint-1360\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.604104104104104e-05, 'epoch': 34.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1360\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1360\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1360\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1280] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1400\n",
      "Configuration saved in model/model_bart\\checkpoint-1400\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.504004004004004e-05, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1400\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1400\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1400\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1320] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1440\n",
      "Configuration saved in model/model_bart\\checkpoint-1440\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 1.403903903903904e-05, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1440\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1440\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1440\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1360] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1480\n",
      "Configuration saved in model/model_bart\\checkpoint-1480\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'learning_rate': 1.3038038038038039e-05, 'epoch': 37.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1480\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1480\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1480\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1400] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1520\n",
      "Configuration saved in model/model_bart\\checkpoint-1520\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 1.2037037037037037e-05, 'epoch': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1520\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1520\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1520\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1440] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1560\n",
      "Configuration saved in model/model_bart\\checkpoint-1560\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.1036036036036037e-05, 'epoch': 39.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1560\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1560\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1560\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1480] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1600\n",
      "Configuration saved in model/model_bart\\checkpoint-1600\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.0035035035035035e-05, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1600\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1600\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1600\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1520] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1640\n",
      "Configuration saved in model/model_bart\\checkpoint-1640\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 9.034034034034034e-06, 'epoch': 41.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1640\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1640\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1640\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1560] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1680\n",
      "Configuration saved in model/model_bart\\checkpoint-1680\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 8.033033033033032e-06, 'epoch': 42.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1680\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1680\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1680\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1600] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1720\n",
      "Configuration saved in model/model_bart\\checkpoint-1720\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 7.032032032032032e-06, 'epoch': 43.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1720\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1720\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1720\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1640] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1760\n",
      "Configuration saved in model/model_bart\\checkpoint-1760\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 6.031031031031031e-06, 'epoch': 44.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1760\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1760\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1760\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1680] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1800\n",
      "Configuration saved in model/model_bart\\checkpoint-1800\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0002, 'learning_rate': 5.03003003003003e-06, 'epoch': 45.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1800\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1800\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1800\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1720] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1840\n",
      "Configuration saved in model/model_bart\\checkpoint-1840\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 4.0290290290290296e-06, 'epoch': 46.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1840\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1840\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1840\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1760] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1880\n",
      "Configuration saved in model/model_bart\\checkpoint-1880\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 3.0280280280280284e-06, 'epoch': 47.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1880\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1880\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1880\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1800] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1920\n",
      "Configuration saved in model/model_bart\\checkpoint-1920\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 2.0270270270270273e-06, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1920\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1920\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1920\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1840] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-1960\n",
      "Configuration saved in model/model_bart\\checkpoint-1960\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 1.0260260260260261e-06, 'epoch': 49.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-1960\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-1960\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-1960\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1880] due to args.save_total_limit\n",
      "Saving model checkpoint to model/model_bart\\checkpoint-2000\n",
      "Configuration saved in model/model_bart\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0001, 'learning_rate': 2.5025025025025025e-08, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in model/model_bart\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/model_bart\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in model/model_bart\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [model\\model_bart\\checkpoint-1920] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 320.2615, 'train_samples_per_second': 12.49, 'train_steps_per_second': 6.245, 'train_loss': 0.001519653731258586, 'epoch': 50.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.001519653731258586, metrics={'train_runtime': 320.2615, 'train_samples_per_second': 12.49, 'train_steps_per_second': 6.245, 'train_loss': 0.001519653731258586, 'epoch': 50.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: \n",
      "<s>Jenna Ushkowitz has award winner of Josh Sussman. Paul Dini has award winner of Adam Horowitz. Don Cheadle has award winner of<mask>.</s> \n",
      "\n",
      "Expected output: \n",
      "Jenna Ushkowitz has award winner of Josh Sussman. Paul Dini has award winner of Adam Horowitz. Don Cheadle has award winner of Larenz Tate. \n",
      "\n",
      "Model Output: \n",
      "</s><s>Jenna Ushkowitz has award winner of Josh Sussman. Paul Dini has award winners of Adam Horowitz. Don Cheadle has award loser of Larenz Tate.</s>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(\"Sample input: \")\n",
    "    print(tokenizer.decode(data[\"input_ids\"]).replace(\"<pad>\", \"\"), \"\\n\")\n",
    "\n",
    "    print(\"Expected output: \")\n",
    "    print(tokenizer.decode(data[\"labels\"], skip_special_tokens=True), \"\\n\")\n",
    "\n",
    "    print(\"Model Output: \")\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(\n",
    "                data[\"input_ids\"].to(device).reshape(1, -1), max_length=MAX_LENGTH\n",
    "            )[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in model/saves\\config.json\n",
      "Model weights saved in model/saves\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"model/saves\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Transformer Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.xticks(list(range(epochs + 1)))\n",
    "plt.plot(loss_epoch)\n",
    "print(loss_epoch[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(\"Decoder Error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"token error rate\")\n",
    "plt.xticks(\n",
    "    list(map(lambda i: i * epoch_accuracy_frequency, list(range(len(decoder_error)))))\n",
    ")\n",
    "plt.plot(\n",
    "    list(map(lambda i: i * epoch_accuracy_frequency, list(range(len(decoder_error))))),\n",
    "    [i[2] for i in decoder_error],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
