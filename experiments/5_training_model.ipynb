{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS ###\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    \"FB15k-237-DECODE-ONLY-LABEL\": \"data/data_processed/FB15k-237/decode_only_label/\",\n",
    "    \"ALL-DATA-DECODE-ONLY-LABEL\": \"data/data_processed/FB15k_FB15k237_WN18_WN18RR/\",\n",
    "}\n",
    "\n",
    "MODELS = {\n",
    "    \"bart-small\": \"lucadiliello/bart-small\",\n",
    "    \"bart-base\": \"facebook/bart-base\",\n",
    "    \"bart-large\": \"facebook/bart-large\",\n",
    "}\n",
    "\n",
    "# Dataset\n",
    "DATASET = \"FB15k-237-DECODE-ONLY-LABEL\"\n",
    "MODEL = \"bart-small\"\n",
    "MODEL_NAME = MODEL + \"_\" + DATASET\n",
    "MODEL_PATH = f\"models/{MODEL_NAME}\"\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "# Training\n",
    "params = {\n",
    "    # Dir\n",
    "    \"output_dir\": f\"models/{MODEL_NAME}/\",\n",
    "    # Batch\n",
    "    \"per_device_train_batch_size\": 128,\n",
    "    \"per_device_eval_batch_size\": 128,\n",
    "    # Learning rate\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"seed\": 42,\n",
    "    # Epochs\n",
    "    \"num_train_epochs\": 10,\n",
    "    # Logging\n",
    "    \"logging_dir\": f\"models/{MODEL_NAME}/logs\",\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"logging_steps\": 10,\n",
    "    # Evaluation\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"eval_steps\": 1,\n",
    "    # Checkpoint\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_steps\": 2,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"ddp_find_unused_parameters\": False,\n",
    "    \"warmup_steps\": 2,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = (\n",
    "    BartForConditionalGeneration.from_pretrained(MODELS[MODEL], use_cache=False)\n",
    "    .cuda()\n",
    "    .float()\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(MODELS[MODEL])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasetkgc import DatasetKGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds_DEV.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds_DEV.pth\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "epochs = params[\"num_train_epochs\"]\n",
    "epoch_accuracy_frequency = [\"eval_steps\"]\n",
    "lr = params[\"learning_rate\"]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds, valid_ds = (\n",
    "    DataLoader(train_ds, batch_size=2, shuffle=False),\n",
    "    DataLoader(valid_ds, batch_size=2, shuffle=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if checkpoint exists\n",
    "\n",
    "if os.path.exists(f\"{params['output_dir']}loop_trainer/checkpoint/\"):\n",
    "    checkpoint_path = (\n",
    "        f\"{params['output_dir']}loop_trainer/checkpoint/\"\n",
    "        + sorted(os.listdir(f\"{params['output_dir']}loop_trainer/checkpoint/\"))[-1]\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    train_epoch_loss = checkpoint[\"loss\"]\n",
    "    train_losses = checkpoint[\"train_losses\"]\n",
    "    valid_losses = checkpoint[\"valid_losses\"]\n",
    "\n",
    "else:\n",
    "    os.makedirs(f\"{params['output_dir']}/loop_trainer/checkpoint/\", exist_ok=True)\n",
    "    start_epoch = 0\n",
    "    train_epoch_loss = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"num_train_epochs\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788089eb9ea54cbc807d0b8bfcd1e45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8f1a66ecf14e76a8039981ae15287e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 12.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pbar = tqdm(range(start_epoch + 1, params[\"num_train_epochs\"] + 1), desc=\"Epochs\")\n",
    "pbar_steps = tqdm(\n",
    "    total=(params[\"num_train_epochs\"] + 1 - start_epoch + 1) * len(train_ds),\n",
    "    desc=\"Training\",\n",
    ")\n",
    "\n",
    "for epoch in pbar:\n",
    "    pbar.set_description(\"Epoch %s\" % epoch)\n",
    "    pbar.refresh()\n",
    "\n",
    "    # Checkpoint\n",
    "    if epoch % params[\"save_steps\"] == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": train_epoch_loss,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"valid_losses\": valid_losses,\n",
    "            },\n",
    "            f\"{params['output_dir']}/loop_trainer/checkpoint/epoch_{epoch}.pth\",\n",
    "        )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_epoch_loss = 0\n",
    "\n",
    "    for batch in train_ds:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids, attention_mask, labels = (\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"labels\"],\n",
    "        )\n",
    "        label = batch[\"labels\"]\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids.to(device),\n",
    "            labels=label.to(device),\n",
    "            attention_mask=attention_mask.to(device),\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar_steps.update(1)\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "\n",
    "    # Flag to avoid multiple trains\n",
    "    start_epoch += 1\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if epoch % params[\"eval_steps\"] == 0:\n",
    "        valid_loss = 0\n",
    "\n",
    "        for batch in valid_ds:\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"labels\"],\n",
    "            )\n",
    "            label = batch[\"labels\"]\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids.to(device),\n",
    "                labels=label.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    else:\n",
    "        valid_losses.append(0)\n",
    "\n",
    "    pbar.set_postfix(loss=train_epoch_loss)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{params['output_dir']}loop_trainer/trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_ds, valid_ds = torch.load(DATASETS[DATASET] + \"train_ds_DEV.pth\"), torch.load(\n",
    "    DATASETS[DATASET] + \"valid_ds_DEV.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    global loss_values\n",
    "    loss = float(eval_pred[\"loss\"])\n",
    "    loss_values.append(eval_pred)\n",
    "\n",
    "    return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "os.makedirs(params[\"output_dir\"] + \"hf_trainer/\", exist_ok=True)\n",
    "os.makedirs(params[\"output_dir\"] + \"hf_trainer/logs/\", exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Dir\n",
    "    output_dir=params[\"output_dir\"] + \"hf_trainer/checkpoint/\",\n",
    "    # Batch\n",
    "    per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "    # Learning Rate\n",
    "    learning_rate=params[\"learning_rate\"],\n",
    "    seed=params[\"seed\"],\n",
    "    # Epoch\n",
    "    num_train_epochs=params[\"num_train_epochs\"],\n",
    "    # logging\n",
    "    logging_dir=params[\"output_dir\"] + \"hf_trainer/logs\",\n",
    "    logging_strategy=params[\"logging_strategy\"],\n",
    "    logging_steps=params[\"logging_steps\"],\n",
    "    # Evaluation\n",
    "    # evaluation_strategy=params[\"evaluation_strategy\"],\n",
    "    # eval_steps=params[\"eval_steps\"],\n",
    "    # Checkpoint\n",
    "    save_strategy=params[\"save_strategy\"],\n",
    "    save_steps=params[\"save_steps\"],\n",
    "    save_total_limit=params[\"save_total_limit\"],\n",
    "    # pretraining\n",
    "    ddp_find_unused_parameters=params[\"ddp_find_unused_parameters\"],\n",
    "    warmup_steps=params[\"warmup_steps\"],\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    # test\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, padding=\"max_length\", max_length=MAX_LENGTH\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83db53ea839496fbb289f0abb5a6479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 6.00 GiB total capacity; 4.84 GiB already allocated; 0 bytes free; 4.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     trainer\u001b[39m.\u001b[39mtrain(resume_from_checkpoint\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\\venv\\lib\\site-packages\\transformers\\trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1520\u001b[0m )\n\u001b[1;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1526\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\\venv\\lib\\site-packages\\transformers\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1761\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1762\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1763\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1765\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1766\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1767\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1768\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1769\u001b[0m ):\n\u001b[0;32m   1770\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\\venv\\lib\\site-packages\\transformers\\trainer.py:2509\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2506\u001b[0m     loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m-> 2509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m   2510\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_apex:\n\u001b[0;32m   2511\u001b[0m     \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mscale_loss(loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer) \u001b[39mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\\venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Matheus\\Documents\\Git\\knowledge-graph-completion\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB (GPU 0; 6.00 GiB total capacity; 4.84 GiB already allocated; 0 bytes free; 4.90 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Verfiy if checkpoint exists\n",
    "if os.listdir(f\"{params['output_dir']}hf_trainer/checkpoint/\") != []:\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "else:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{params['output_dir']}hf_trainer/trained_model/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(f\"{f'{MODEL} - {DATASET}'} train\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss - Cross Entropy\")\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "\n",
    "plt.text(\n",
    "    max(xmin, xmax) * 0.9,\n",
    "    max(ymin, ymax) * 0.9,\n",
    "    f'epochs = {params[\"num_train_epochs\"]}\\nlr={params[\"learning_rate\"]}',\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i[\"epoch\"] for i in trainer.state.log_history[:-1]]\n",
    "y = [i[\"loss\"] for i in trainer.state.log_history[:-1]]\n",
    "plt.title(f\"{f'{MODEL} - {DATASET}'} Loss\")\n",
    "plt.text(\n",
    "    max(x) * 0.99,\n",
    "    max(y) * 0.99,\n",
    "    f'epochs = {params[\"num_train_epochs\"]}\\nlr={params[\"learning_rate\"]}',\n",
    "    ha=\"right\",\n",
    "    va=\"top\",\n",
    ")\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss - Cross Entropy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
